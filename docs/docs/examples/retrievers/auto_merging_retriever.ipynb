{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b39acfe0",
      "metadata": {
        "id": "b39acfe0"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/retrievers/auto_merging_retriever.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5325ac27-38ea-47aa-afef-be4ec4f8f4b9",
      "metadata": {
        "id": "5325ac27-38ea-47aa-afef-be4ec4f8f4b9"
      },
      "source": [
        "# Auto Merging Retriever\n",
        "\n",
        "In this notebook, we showcase our `AutoMergingRetriever`, which looks at a set of leaf nodes and recursively \"merges\" subsets of leaf nodes that reference a parent node beyond a given threshold. This allows us to consolidate potentially disparate, smaller contexts into a larger context that might help synthesis.\n",
        "\n",
        "You can define this hierarchy yourself over a set of documents, or you can make use of our brand-new text parser: a HierarchicalNodeParser that takes in a candidate set of documents and outputs an entire hierarchy of nodes, from \"coarse-to-fine\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0a15c0d4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a15c0d4",
        "outputId": "393a14f3-c799-4325-e1e6-c8e9b9e122ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index-llms-openai in /usr/local/lib/python3.12/dist-packages (0.5.4)\n",
            "Requirement already satisfied: llama-index-core<0.14,>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-llms-openai) (0.13.3)\n",
            "Requirement already satisfied: openai<2,>=1.81.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-llms-openai) (1.102.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (3.12.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (2025.7.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (1.3.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (11.3.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (4.3.8)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (2.0.43)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (0.11.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (4.15.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (1.17.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.81.0->llama-index-llms-openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2,>=1.81.0->llama-index-llms-openai) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.81.0->llama-index-llms-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2,>=1.81.0->llama-index-llms-openai) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2,>=1.81.0->llama-index-llms-openai) (3.10)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (1.13.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (3.1.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (0.16.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (2025.7.34)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (3.26.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (25.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-llms-openai) (3.0.2)\n",
            "Requirement already satisfied: llama-index-readers-file in /usr/local/lib/python3.12/dist-packages (0.5.2)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file) (4.13.5)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file) (0.7.1)\n",
            "Requirement already satisfied: llama-index-core<0.14,>=0.13.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file) (0.13.3)\n",
            "Requirement already satisfied: pandas<2.3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file) (2.2.2)\n",
            "Requirement already satisfied: pypdf<7,>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file) (6.0.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file) (0.0.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file) (4.15.0)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (3.12.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (2025.7.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (1.3.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (11.3.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (4.3.8)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (2.0.43)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (0.11.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (4.67.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (1.17.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (1.20.1)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (1.13.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (3.1.6)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (2025.7.34)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (3.26.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (0.16.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (25.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (1.3.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.0->llama-index-readers-file) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index-llms-openai\n",
        "%pip install llama-index-readers-file pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7ee0e185-408a-4c9c-a361-b6af96129b0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "7ee0e185-408a-4c9c-a361-b6af96129b0d",
        "outputId": "c09184a4-5ed0-4bb4-d149-b5de27a82b0e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'imp'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-329368314.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoreload'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magics/extension.py\u001b[0m in \u001b[0;36mload_ext\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodule_str\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUsageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Missing module name.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextension_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'already loaded'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/extensions.py\u001b[0m in \u001b[0;36mload_extension\u001b[0;34m(self, module_str)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodule_str\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mprepended_to_syspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                     \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_extension_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                         print((\"Loading extensions from {dir} is deprecated. \"\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/extensions/autoreload.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msource_from_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;31m#------------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imp'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9fbdc26",
      "metadata": {
        "id": "b9fbdc26"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "06c89955",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06c89955",
        "outputId": "b0961626-24ec-4692-a766-15ee01890b3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting llama-index-cli<0.6,>=0.5.0 (from llama-index)\n",
            "  Downloading llama_index_cli-0.5.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: llama-index-core<0.14,>=0.13.3 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.13.3)\n",
            "Collecting llama-index-embeddings-openai<0.6,>=0.5.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.5.0-py3-none-any.whl.metadata (400 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.9.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.5.4)\n",
            "Requirement already satisfied: llama-index-readers-file<0.6,>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index) (0.5.2)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.5.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (3.12.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (2025.7.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (1.3.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (11.3.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (4.3.8)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.3->llama-index) (2.0.43)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (9.1.2)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (0.11.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (4.15.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.14,>=0.13.3->llama-index) (1.17.3)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.102.0)\n",
            "Collecting llama-cloud==0.1.35 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.35-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.12/dist-packages (from llama-cloud==0.1.35->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.8.3)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (4.13.5)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.7.1)\n",
            "Requirement already satisfied: pandas<2.3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.2.2)\n",
            "Requirement already satisfied: pypdf<7,>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (6.0.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.12/dist-packages (from llama-index-readers-file<0.6,>=0.5.0->llama-index) (0.0.26)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.62-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index) (2025.7.34)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.3->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.3->llama-index) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.3->llama-index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.3->llama-index) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.3->llama-index) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.3->llama-index) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.14,>=0.13.3->llama-index) (1.20.1)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.3->llama-index) (1.13.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.3->llama-index) (3.1.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.14,>=0.13.3->llama-index) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.14,>=0.13.3->llama-index) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.14,>=0.13.3->llama-index) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.14,>=0.13.3->llama-index) (0.16.0)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.14,>=0.13.3->llama-index) (0.4.0)\n",
            "Collecting llama-cloud-services>=0.6.62 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.62-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.1.0->llama-index-embeddings-openai<0.6,>=0.5.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.3->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.3->llama-index) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.14,>=0.13.3->llama-index) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.3->llama-index) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.14,>=0.13.3->llama-index) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.14,>=0.13.3->llama-index) (3.2.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.14,>=0.13.3->llama-index) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.14,>=0.13.3->llama-index) (3.26.1)\n",
            "INFO: pip is looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.60-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.60 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.60-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.59-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.59 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.59-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.58-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: pip is still looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting llama-cloud-services>=0.6.58 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.58-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.57-py3-none-any.whl.metadata (6.6 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "Collecting llama-cloud-services>=0.6.56 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.57-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading llama_cloud_services-0.6.56-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.56-py3-none-any.whl.metadata (6.6 kB)\n",
            "  Downloading llama_parse-0.6.55-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.55 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.55-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.54-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting llama-cloud-services>=0.6.54 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.54-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv<2,>=1.0.1 (from llama-cloud-services>=0.6.54->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.14,>=0.13.3->llama-index) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.6,>=0.5.0->llama-index) (1.17.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.3->llama-index) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.14,>=0.13.3->llama-index) (3.0.2)\n",
            "Downloading llama_index-0.13.3-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_cli-0.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_embeddings_openai-0.5.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.9.2-py3-none-any.whl (16 kB)\n",
            "Downloading llama_cloud-0.1.35-py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.5.0-py3-none-any.whl (3.2 kB)\n",
            "Downloading llama_parse-0.6.54-py3-none-any.whl (4.9 kB)\n",
            "Downloading llama_cloud_services-0.6.54-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, llama-cloud, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-cli, llama-index-readers-llama-parse, llama-index\n",
            "Successfully installed llama-cloud-0.1.35 llama-cloud-services-0.6.54 llama-index-0.13.3 llama-index-cli-0.5.0 llama-index-embeddings-openai-0.5.0 llama-index-indices-managed-llama-cloud-0.9.2 llama-index-readers-llama-parse-0.5.0 llama-parse-0.6.54 python-dotenv-1.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e1316ac-84ca-41d0-80f9-d4ef758e653c",
      "metadata": {
        "id": "7e1316ac-84ca-41d0-80f9-d4ef758e653c"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "Let's first load the Llama 2 paper: https://arxiv.org/pdf/2307.09288.pdf. This will be our test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "80372299-ab32-4ddd-9b88-05c877120c17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80372299-ab32-4ddd-9b88-05c877120c17",
        "outputId": "f4f1a6ca-b95a-4e76-9507-15d5247ad163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-30 09:13:11--  https://arxiv.org/pdf/2307.09288.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.3.42, 151.101.131.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.3.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /pdf/2307.09288 [following]\n",
            "--2025-08-30 09:13:11--  https://arxiv.org/pdf/2307.09288\n",
            "Reusing existing connection to arxiv.org:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13661300 (13M) [application/pdf]\n",
            "Saving to: â€˜data/llama2.pdfâ€™\n",
            "\n",
            "\rdata/llama2.pdf       0%[                    ]       0  --.-KB/s               \rdata/llama2.pdf     100%[===================>]  13.03M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-08-30 09:13:11 (119 MB/s) - â€˜data/llama2.pdfâ€™ saved [13661300/13661300]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p 'data/'\n",
        "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5f9c5d99-bd0e-4b26-b816-9f5ad29df3c8",
      "metadata": {
        "id": "5f9c5d99-bd0e-4b26-b816-9f5ad29df3c8"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "from llama_index.readers.file import PDFReader\n",
        "from llama_index.readers.file import PyMuPDFReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "723f1f02-2157-4166-b013-90e627c76530",
      "metadata": {
        "id": "723f1f02-2157-4166-b013-90e627c76530"
      },
      "outputs": [],
      "source": [
        "loader = PyMuPDFReader()\n",
        "# docs0 = loader.load_data(file=Path(\"./data/llama2.pdf\"))\n",
        "docs0 = loader.load(file_path=Path(\"./data/llama2.pdf\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff7a8552-f347-45b0-b4a0-4f9b32be57ac",
      "metadata": {
        "id": "ff7a8552-f347-45b0-b4a0-4f9b32be57ac"
      },
      "source": [
        "By default, the PDF reader creates a separate doc for each page.\n",
        "For the sake of this notebook, we stitch docs together into one doc.\n",
        "This will help us better highlight auto-merging capabilities that \"stitch\" chunks together later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a75c4217-ab50-417f-a8ed-3b746a9956c8",
      "metadata": {
        "id": "a75c4217-ab50-417f-a8ed-3b746a9956c8"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Document\n",
        "\n",
        "doc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\n",
        "docs = [Document(text=doc_text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "724fe6f1-80e1-4ac5-bd99-8b9b8d15bddd",
      "metadata": {
        "id": "724fe6f1-80e1-4ac5-bd99-8b9b8d15bddd"
      },
      "source": [
        "## Parse Chunk Hierarchy from Text, Load into Storage\n",
        "\n",
        "In this section we make use of the `HierarchicalNodeParser`. This will output a hierarchy of nodes, from top-level nodes with bigger chunk sizes to child nodes with smaller chunk sizes, where each child node has a parent node with a bigger chunk size.\n",
        "\n",
        "By default, the hierarchy is:\n",
        "- 1st level: chunk size 2048\n",
        "- 2nd level: chunk size 512\n",
        "- 3rd level: chunk size 128\n",
        "\n",
        "\n",
        "We then load these nodes into storage. The leaf nodes are indexed and retrieved via a vector store - these are the nodes that will first be directly retrieved via similarity search. The other nodes will be retrieved from a docstore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "45e783f5-a323-4f51-ae9a-4b71b00e5e11",
      "metadata": {
        "id": "45e783f5-a323-4f51-ae9a-4b71b00e5e11"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import (\n",
        "    HierarchicalNodeParser,\n",
        "    SentenceSplitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2c3947df-25c2-4254-a3d4-381d136f3f77",
      "metadata": {
        "id": "2c3947df-25c2-4254-a3d4-381d136f3f77"
      },
      "outputs": [],
      "source": [
        "node_parser = HierarchicalNodeParser.from_defaults()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2162b309-dfc5-484b-a31c-24f705316f10",
      "metadata": {
        "id": "2162b309-dfc5-484b-a31c-24f705316f10"
      },
      "outputs": [],
      "source": [
        "nodes = node_parser.get_nodes_from_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a9b5bc9b-389d-47db-a41c-3eb5b3d38ac5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9b5bc9b-389d-47db-a41c-3eb5b3d38ac5",
        "outputId": "0c83868f-00d6-4efa-d5f9-46ee9d762292"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1009"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "len(nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7456b70-1803-4786-86d5-26e202e0f318",
      "metadata": {
        "id": "a7456b70-1803-4786-86d5-26e202e0f318"
      },
      "source": [
        "Here we import a simple helper function for fetching \"leaf\" nodes within a node list.\n",
        "These are nodes that don't have children of their own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7299ca7e-09b6-432f-a277-aae9eca0522a",
      "metadata": {
        "id": "7299ca7e-09b6-432f-a277-aae9eca0522a"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import get_leaf_nodes, get_root_nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "faeb37a8-aea9-4ee8-b6c0-3b2f188d244e",
      "metadata": {
        "id": "faeb37a8-aea9-4ee8-b6c0-3b2f188d244e"
      },
      "outputs": [],
      "source": [
        "leaf_nodes = get_leaf_nodes(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7c33b5a8-4d9f-481e-8616-fc8717900159",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c33b5a8-4d9f-481e-8616-fc8717900159",
        "outputId": "d9b55dc7-bf7a-4ea7-ac2c-17283d77d526"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "783"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(leaf_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b28b0aff-db6e-495e-8c58-36db29edb45b",
      "metadata": {
        "id": "b28b0aff-db6e-495e-8c58-36db29edb45b"
      },
      "outputs": [],
      "source": [
        "root_nodes = get_root_nodes(nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c36ec940-8af7-45f5-9994-919d57583c24",
      "metadata": {
        "id": "c36ec940-8af7-45f5-9994-919d57583c24"
      },
      "source": [
        "### Load into Storage\n",
        "\n",
        "We define a docstore, which we load all nodes into.\n",
        "\n",
        "We then define a `VectorStoreIndex` containing just the leaf-level nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "27c8f2cd-3e04-4feb-937b-b9ee33e1c2fd",
      "metadata": {
        "id": "27c8f2cd-3e04-4feb-937b-b9ee33e1c2fd"
      },
      "outputs": [],
      "source": [
        "# define storage context\n",
        "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
        "from llama_index.core import StorageContext\n",
        "# from llama_index.llms.openai import OpenAI # Comment out OpenAI import\n",
        "from llama_index.llms.ollama import Ollama # Import Ollama for local LLM\n",
        "\n",
        "docstore = SimpleDocumentStore()\n",
        "\n",
        "# insert nodes into docstore\n",
        "docstore.add_documents(nodes)\n",
        "\n",
        "# define storage context (will include vector store by default too)\n",
        "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
        "\n",
        "# llm = OpenAI(model=\"gpt-3.5-turbo\") # Comment out OpenAI LLM\n",
        "llm = Ollama(model=\"llama2\") # Use Ollama with llama2 as an example local LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "827ece8e-7a4b-4ee1-8ee2-3433d7f2072a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "827ece8e-7a4b-4ee1-8ee2-3433d7f2072a",
        "outputId": "2f135add-ca48-4443-8fe3-c8a64f14d98d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "\n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llama_index/core/embeddings/utils.py\u001b[0m in \u001b[0;36mresolve_embed_model\u001b[0;34m(embed_model, callback_manager)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0membed_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAIEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mvalidate_openai_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llama_index/embeddings/openai/utils.py\u001b[0m in \u001b[0;36mvalidate_openai_api_key\u001b[0;34m(api_key)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopenai_api_key\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_API_KEY_ERROR_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: No API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2890505924.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorStoreIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m base_index = VectorStoreIndex(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mleaf_nodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstorage_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llama_index/core/indices/vector_store/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nodes, use_async, store_nodes_override, embed_model, insert_batch_size, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_store_nodes_override\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore_nodes_override\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         self._embed_model = resolve_embed_model(\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0membed_model\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         )\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llama_index/core/settings.py\u001b[0m in \u001b[0;36membed_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;34m\"\"\"Get the embedding model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embed_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embed_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve_embed_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llama_index/core/embeddings/utils.py\u001b[0m in \u001b[0;36mresolve_embed_model\u001b[0;34m(embed_model, callback_manager)\u001b[0m\n\u001b[1;32m     64\u001b[0m             )\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0;34m\"\\n******\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;34m\"Could not load OpenAI embedding model. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \n******\nCould not load OpenAI embedding model. If you intended to use OpenAI, please check your OPENAI_API_KEY.\nOriginal error:\nNo API key found for OpenAI.\nPlease set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.\nAPI keys can be found or created at https://platform.openai.com/account/api-keys\n\nConsider using embed_model='local'.\nVisit our documentation for more embedding options: https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings.html#modules\n******"
          ]
        }
      ],
      "source": [
        "## Load index into vector index\n",
        "from llama_index.core import VectorStoreIndex\n",
        "\n",
        "base_index = VectorStoreIndex(\n",
        "    leaf_nodes,\n",
        "    storage_context=storage_context,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05d84c19-c9ac-4294-a000-264c3c02427b",
      "metadata": {
        "id": "05d84c19-c9ac-4294-a000-264c3c02427b"
      },
      "source": [
        "## Define Retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e61682a0-dd3c-400b-8734-35d5d0a98252",
      "metadata": {
        "id": "e61682a0-dd3c-400b-8734-35d5d0a98252"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.retrievers import AutoMergingRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f96fd0bc-c6c0-4073-a692-d1803cf4289f",
      "metadata": {
        "id": "f96fd0bc-c6c0-4073-a692-d1803cf4289f"
      },
      "outputs": [],
      "source": [
        "base_retriever = base_index.as_retriever(similarity_top_k=6)\n",
        "retriever = AutoMergingRetriever(base_retriever, storage_context, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62f655cd-4195-4398-80e5-5aa561982d25",
      "metadata": {
        "id": "62f655cd-4195-4398-80e5-5aa561982d25",
        "outputId": "2924cbce-f0f2-4d9d-dcb9-f2b3a49512c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Merging 4 nodes into parent node.\n",
            "> Parent node id: caf5f81c-842f-46a4-b679-6be584bd6aff.\n",
            "> Parent node text: We conduct RLHF by first collecting human preference data for safety similar to Section 3.2.2: an...\n"
          ]
        }
      ],
      "source": [
        "# query_str = \"What were some lessons learned from red-teaming?\"\n",
        "# query_str = \"Can you tell me about the key concepts for safety finetuning\"\n",
        "query_str = (\n",
        "    \"What could be the potential outcomes of adjusting the amount of safety\"\n",
        "    \" data used in the RLHF stage?\"\n",
        ")\n",
        "\n",
        "nodes = retriever.retrieve(query_str)\n",
        "base_nodes = base_retriever.retrieve(query_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77eabc56-2009-4504-8832-b6d857bd43a4",
      "metadata": {
        "id": "77eabc56-2009-4504-8832-b6d857bd43a4",
        "outputId": "d5401d95-30cc-4179-bb66-80424d45b153"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a82690a-263a-4e48-ab27-b161e72cb983",
      "metadata": {
        "id": "3a82690a-263a-4e48-ab27-b161e72cb983",
        "outputId": "7da70313-e491-4a99-8824-7bf5fc9c0114"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(base_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d482b22-fd38-476b-821f-0c77564815c3",
      "metadata": {
        "id": "0d482b22-fd38-476b-821f-0c77564815c3",
        "outputId": "74671d35-1258-46db-8391-5ebe0ea80036"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Node ID:** d4d67180-71c8-4328-b3f1-1e98fa42ab69<br>**Similarity:** 0.8694979150607424<br>**Text:** We also list two\n",
              "qualitative examples where safety and helpfulness reward models donâ€™t agree with each other in Table 35.\n",
              "A.4.2\n",
              "Qualitative Results on Safety Data Scaling\n",
              "In Section 4.2.3, we study the impact of adding more safety data into model RLHF in a quantitative manner.\n",
              "Here we showcase a few samples to qualitatively examine the evolution of model behavior when we scale\n",
              "safety data in Tables 36, 37, and 38. In general, we are observing that Llama 2-Chat becomes safer responding\n",
              "to unsafe prompts with more safety data used.<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Node ID:** caf5f81c-842f-46a4-b679-6be584bd6aff<br>**Similarity:** 0.86168727941324<br>**Text:** We conduct RLHF by first collecting human preference data for safety similar to Section 3.2.2: annotators\n",
              "write a prompt that they believe can elicit unsafe behavior, and then compare multiple model responses to\n",
              "the prompts, selecting the response that is safest according to a set of guidelines. We then use the human\n",
              "preference data to train a safety reward model (see Section 3.2.2), and also reuse the adversarial prompts to\n",
              "sample from the model during the RLHF stage.\n",
              "Better Long-Tail Safety Robustness without Hurting Helpfulness\n",
              "Safety is inherently a long-tail problem,\n",
              "where the challenge comes from a small number of very specific cases. We investigate the impact of Safety\n",
              "RLHF by taking two intermediate Llama 2-Chat checkpointsâ€”one without adversarial prompts in the RLHF\n",
              "stage and one with themâ€”and score their responses on our test sets using our safety and helpfulness reward\n",
              "models. In Figure 14, we plot the score distribution shift of the safety RM on the safety test set (left) and that\n",
              "of the helpfulness RM on the helpfulness test set (right). In the left hand side of the figure, we observe that\n",
              "the distribution of safety RM scores on the safety set shifts to higher reward scores after safety tuning with\n",
              "RLHF, and that the long tail of the distribution near zero thins out. A clear cluster appears on the top-left\n",
              "corner suggesting the improvements of model safety. On the right side, we do not observe any gathering\n",
              "pattern below the y = x line on the right hand side of Figure 14, which indicates that the helpfulness score\n",
              "distribution is preserved after safety tuning with RLHF. Put another way, given sufficient helpfulness training\n",
              "data, the addition of an additional stage of safety mitigation does not negatively impact model performance\n",
              "on helpfulness to any notable degradation. A qualitative example is shown in Table 12.\n",
              "Impact of Safety Data Scaling.\n",
              "A tension between helpfulness and safety of LLMs has been observed in\n",
              "previous studies (Bai et al., 2022a). To better understand how the addition of safety training data affects\n",
              "general model performance, especially helpfulness, we investigate the trends in safety data scaling by\n",
              "adjusting the amount of safety data used in the RLHF stage.<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Node ID:** d9893bef-a5a7-4248-a0a1-d7c28800ae59<br>**Similarity:** 0.8546977459150967<br>**Text:** 0\n",
              "0.2\n",
              "0.4\n",
              "0.6\n",
              "0.8\n",
              "1.0\n",
              "Helpfulness RM Score before Safety RLHF\n",
              "0.0\n",
              "0.2\n",
              "0.4\n",
              "0.6\n",
              "0.8\n",
              "1.0\n",
              "Helpfulness RM Score after Safety RLHF\n",
              "0\n",
              "1000\n",
              "0\n",
              "1000\n",
              "Figure 14: Impact of safety RLHF measured by reward model score distributions. Left: safety reward\n",
              "model scores of generations on the Meta Safety test set. The clustering of samples in the top left corner\n",
              "suggests the improvements of model safety.<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from llama_index.core.response.notebook_utils import display_source_node\n",
        "\n",
        "for node in nodes:\n",
        "    display_source_node(node, source_length=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4dd58db-4b12-49dc-b42f-8a0ee746f5c9",
      "metadata": {
        "id": "e4dd58db-4b12-49dc-b42f-8a0ee746f5c9",
        "outputId": "35362b37-8b71-41e2-a0d8-860aaa64487e"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Node ID:** 16328561-9ff7-4307-8d31-adf6bb74b71b<br>**Similarity:** 0.8770715326726375<br>**Text:** A qualitative example is shown in Table 12.\n",
              "Impact of Safety Data Scaling.\n",
              "A tension between helpfulness and safety of LLMs has been observed in\n",
              "previous studies (Bai et al., 2022a). To better understand how the addition of safety training data affects\n",
              "general model performance, especially helpfulness, we investigate the trends in safety data scaling by\n",
              "adjusting the amount of safety data used in the RLHF stage.<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Node ID:** e756d327-1a28-4228-ac38-f8a831b1bf77<br>**Similarity:** 0.8728111844788112<br>**Text:** A clear cluster appears on the top-left\n",
              "corner suggesting the improvements of model safety. On the right side, we do not observe any gathering\n",
              "pattern below the y = x line on the right hand side of Figure 14, which indicates that the helpfulness score\n",
              "distribution is preserved after safety tuning with RLHF. Put another way, given sufficient helpfulness training\n",
              "data, the addition of an additional stage of safety mitigation does not negatively impact model performance\n",
              "on helpfulness to any notable degradation. A qualitative example is shown in Table 12.\n",
              "Impact of Safety Data Scaling.<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Node ID:** d4d67180-71c8-4328-b3f1-1e98fa42ab69<br>**Similarity:** 0.8697379697028405<br>**Text:** We also list two\n",
              "qualitative examples where safety and helpfulness reward models donâ€™t agree with each other in Table 35.\n",
              "A.4.2\n",
              "Qualitative Results on Safety Data Scaling\n",
              "In Section 4.2.3, we study the impact of adding more safety data into model RLHF in a quantitative manner.\n",
              "Here we showcase a few samples to qualitatively examine the evolution of model behavior when we scale\n",
              "safety data in Tables 36, 37, and 38. In general, we are observing that Llama 2-Chat becomes safer responding\n",
              "to unsafe prompts with more safety data used.<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Node ID:** d9893bef-a5a7-4248-a0a1-d7c28800ae59<br>**Similarity:** 0.855087365309258<br>**Text:** 0\n",
              "0.2\n",
              "0.4\n",
              "0.6\n",
              "0.8\n",
              "1.0\n",
              "Helpfulness RM Score before Safety RLHF\n",
              "0.0\n",
              "0.2\n",
              "0.4\n",
              "0.6\n",
              "0.8\n",
              "1.0\n",
              "Helpfulness RM Score after Safety RLHF\n",
              "0\n",
              "1000\n",
              "0\n",
              "1000\n",
              "Figure 14: Impact of safety RLHF measured by reward model score distributions. Left: safety reward\n",
              "model scores of generations on the Meta Safety test set. The clustering of samples in the top left corner\n",
              "suggests the improvements of model safety.<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Node ID:** d62ee107-9841-44b5-8b70-bc6487ad6315<br>**Similarity:** 0.8492541852986794<br>**Text:** Better Long-Tail Safety Robustness without Hurting Helpfulness\n",
              "Safety is inherently a long-tail problem,\n",
              "where the challenge comes from a small number of very specific cases. We investigate the impact of Safety\n",
              "RLHF by taking two intermediate Llama 2-Chat checkpointsâ€”one without adversarial prompts in the RLHF\n",
              "stage and one with themâ€”and score their responses on our test sets using our safety and helpfulness reward\n",
              "models.<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "**Node ID:** 312a63b3-5e28-4fbf-a3e1-4e8dc0c026ea<br>**Similarity:** 0.8488371951811564<br>**Text:** We conduct RLHF by first collecting human preference data for safety similar to Section 3.2.2: annotators\n",
              "write a prompt that they believe can elicit unsafe behavior, and then compare multiple model responses to\n",
              "the prompts, selecting the response that is safest according to a set of guidelines. We then use the human\n",
              "preference data to train a safety reward model (see Section 3.2.2), and also reuse the adversarial prompts to\n",
              "sample from the model during the RLHF stage.<br>"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for node in base_nodes:\n",
        "    display_source_node(node, source_length=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f62e2c-4def-402e-8904-47f34d12c2fb",
      "metadata": {
        "id": "08f62e2c-4def-402e-8904-47f34d12c2fb"
      },
      "source": [
        "## Plug it into Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d3ce9ec-f6cd-475b-94fa-3e8df81ab824",
      "metadata": {
        "id": "5d3ce9ec-f6cd-475b-94fa-3e8df81ab824"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_engine import RetrieverQueryEngine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f106e1bb-58bc-48bf-a46b-e527339f83c5",
      "metadata": {
        "id": "f106e1bb-58bc-48bf-a46b-e527339f83c5"
      },
      "outputs": [],
      "source": [
        "query_engine = RetrieverQueryEngine.from_args(retriever)\n",
        "base_query_engine = RetrieverQueryEngine.from_args(base_retriever)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94a85854-ca04-41ed-9f44-b6dce1e513e1",
      "metadata": {
        "id": "94a85854-ca04-41ed-9f44-b6dce1e513e1",
        "outputId": "27f5176c-41b6-41e7-ec9a-e7c17e8fed5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Merging 4 nodes into parent node.\n",
            "> Parent node id: 3671b20d-ea5e-4afc-983e-02be6ee8302d.\n",
            "> Parent node text: We conduct RLHF by first collecting human preference data for safety similar to Section 3.2.2: an...\n"
          ]
        }
      ],
      "source": [
        "response = query_engine.query(query_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b334b7b-fcb8-4057-a418-b8d8c425ad14",
      "metadata": {
        "id": "8b334b7b-fcb8-4057-a418-b8d8c425ad14",
        "outputId": "90785c38-cd82-4c9d-b495-3a2c5cfc97c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjusting the amount of safety data used in the RLHF stage could potentially have the following outcomes:\n",
            "1. Improved model safety: Increasing the amount of safety data used in RLHF may lead to improvements in model safety. This means that the model becomes better at responding to unsafe prompts and avoids generating unsafe or harmful outputs.\n",
            "2. Thinning out of the long tail of safety RM scores: Increasing the amount of safety data may result in a shift in the distribution of safety reward model (RM) scores towards higher reward scores. This means that the model becomes more consistent in generating safe responses and reduces the occurrence of low safety scores.\n",
            "3. Preservation of helpfulness performance: Adjusting the amount of safety data used in RLHF is not expected to negatively impact model performance on helpfulness. This means that the model's ability to generate helpful responses is maintained even after incorporating additional safety training.\n",
            "4. Gathering pattern in helpfulness RM scores: There is no observed gathering pattern below the y = x line in the distribution of helpfulness RM scores after safety tuning with RLHF. This suggests that the helpfulness score distribution is preserved, indicating that the model's helpfulness performance is not significantly degraded by the addition of safety mitigation measures.\n",
            "Overall, adjusting the amount of safety data used in the RLHF stage aims to strike a balance between improving model safety without compromising its helpfulness performance.\n"
          ]
        }
      ],
      "source": [
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c38a124-5279-4a43-a2fe-ed2cbce9bd66",
      "metadata": {
        "id": "1c38a124-5279-4a43-a2fe-ed2cbce9bd66"
      },
      "outputs": [],
      "source": [
        "base_response = base_query_engine.query(query_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c2910e5-1a45-4de5-8035-5b5a47125d81",
      "metadata": {
        "id": "5c2910e5-1a45-4de5-8035-5b5a47125d81",
        "outputId": "0f89fb47-050c-4e85-acb7-ab88e6b1278a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjusting the amount of safety data used in the RLHF stage could potentially lead to improvements in model safety. This can be observed by a clear cluster appearing on the top-left corner, suggesting enhanced model safety. Additionally, it is indicated that the helpfulness score distribution is preserved after safety tuning with RLHF, indicating that the addition of safety data does not negatively impact model performance on helpfulness.\n"
          ]
        }
      ],
      "source": [
        "print(str(base_response))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f84be450-c036-4cca-bf94-ccfc18e5d52a",
      "metadata": {
        "id": "f84be450-c036-4cca-bf94-ccfc18e5d52a"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "We evaluate how well the hierarchical retriever works compared to the baseline retriever in a more quantitative manner.\n",
        "\n",
        "**WARNING**: This can be *expensive*, especially with GPT-4. Use caution and tune the sample size to fit your budget."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb5a6511-5756-4cdb-933e-f530f0c40bc3",
      "metadata": {
        "id": "cb5a6511-5756-4cdb-933e-f530f0c40bc3"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.evaluation import DatasetGenerator, QueryResponseDataset\n",
        "from llama_index.llms.openai import OpenAI\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "914e3056-d9e3-42a0-9600-a66ae6a9f075",
      "metadata": {
        "id": "914e3056-d9e3-42a0-9600-a66ae6a9f075"
      },
      "outputs": [],
      "source": [
        "# NOTE: run this if the dataset isn't already saved\n",
        "# Note: we only generate from the first 20 nodes, since the rest are references\n",
        "eval_llm = OpenAI(model=\"gpt-4\")\n",
        "dataset_generator = DatasetGenerator(\n",
        "    root_nodes[:20],\n",
        "    llm=eval_llm,\n",
        "    show_progress=True,\n",
        "    num_questions_per_chunk=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9569b69-d9bf-4b85-a1b3-3ff4ef6619b8",
      "metadata": {
        "id": "d9569b69-d9bf-4b85-a1b3-3ff4ef6619b8"
      },
      "outputs": [],
      "source": [
        "eval_dataset = await dataset_generator.agenerate_dataset_from_nodes(num=60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5b3ba74-0092-4906-88cc-638fa304c97b",
      "metadata": {
        "id": "e5b3ba74-0092-4906-88cc-638fa304c97b"
      },
      "outputs": [],
      "source": [
        "eval_dataset.save_json(\"data/llama2_eval_qr_dataset.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87f30894-ba65-4af7-9380-210f6a5b2de4",
      "metadata": {
        "id": "87f30894-ba65-4af7-9380-210f6a5b2de4"
      },
      "outputs": [],
      "source": [
        "# optional\n",
        "eval_dataset = QueryResponseDataset.from_json(\n",
        "    \"data/llama2_eval_qr_dataset.json\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d793ae5-80be-41ff-8b1f-e92ea27b2a8b",
      "metadata": {
        "id": "2d793ae5-80be-41ff-8b1f-e92ea27b2a8b"
      },
      "source": [
        "### Compare Results\n",
        "\n",
        "We run evaluations on each of the retrievers: correctness, semantic similarity, relevance, and faithfulness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90ca35cf-e659-4a1e-8561-d07d50972b3a",
      "metadata": {
        "id": "90ca35cf-e659-4a1e-8561-d07d50972b3a"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6814643-bdb2-47cd-a8a5-69a1bdfdda30",
      "metadata": {
        "id": "f6814643-bdb2-47cd-a8a5-69a1bdfdda30"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.evaluation import (\n",
        "    CorrectnessEvaluator,\n",
        "    SemanticSimilarityEvaluator,\n",
        "    RelevancyEvaluator,\n",
        "    FaithfulnessEvaluator,\n",
        "    PairwiseComparisonEvaluator,\n",
        ")\n",
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "# NOTE: can uncomment other evaluators\n",
        "evaluator_c = CorrectnessEvaluator(llm=eval_llm)\n",
        "evaluator_s = SemanticSimilarityEvaluator(llm=eval_llm)\n",
        "evaluator_r = RelevancyEvaluator(llm=eval_llm)\n",
        "evaluator_f = FaithfulnessEvaluator(llm=eval_llm)\n",
        "# pairwise_evaluator = PairwiseComparisonEvaluator(llm=eval_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae472816-5927-4f67-9105-fd0ba0c60f49",
      "metadata": {
        "id": "ae472816-5927-4f67-9105-fd0ba0c60f49"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.evaluation.eval_utils import (\n",
        "    get_responses,\n",
        "    get_results_df,\n",
        ")\n",
        "from llama_index.core.evaluation import BatchEvalRunner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdfd6d5b-7c73-40a5-9636-8db6a248ac00",
      "metadata": {
        "id": "fdfd6d5b-7c73-40a5-9636-8db6a248ac00"
      },
      "outputs": [],
      "source": [
        "eval_qs = eval_dataset.questions\n",
        "qr_pairs = eval_dataset.qr_pairs\n",
        "ref_response_strs = [r for (_, r) in qr_pairs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7302e7b-6b3e-4d25-874b-94ffe944b527",
      "metadata": {
        "id": "a7302e7b-6b3e-4d25-874b-94ffe944b527"
      },
      "outputs": [],
      "source": [
        "pred_responses = get_responses(eval_qs, query_engine, show_progress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bce562f-6c42-446a-b991-f208ca9f55cb",
      "metadata": {
        "id": "0bce562f-6c42-446a-b991-f208ca9f55cb",
        "outputId": "d77a73b8-cf00-4be7-8ff2-e1542e28a9fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:07<00:00,  8.17it/s]\n"
          ]
        }
      ],
      "source": [
        "base_pred_responses = get_responses(\n",
        "    eval_qs, base_query_engine, show_progress=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3428afe0-b5e4-4604-b7ce-270f449766cb",
      "metadata": {
        "id": "3428afe0-b5e4-4604-b7ce-270f449766cb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "pred_response_strs = [str(p) for p in pred_responses]\n",
        "base_pred_response_strs = [str(p) for p in base_pred_responses]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0a86029-bf5e-4f26-afdf-a9406bada315",
      "metadata": {
        "id": "b0a86029-bf5e-4f26-afdf-a9406bada315"
      },
      "outputs": [],
      "source": [
        "evaluator_dict = {\n",
        "    \"correctness\": evaluator_c,\n",
        "    \"faithfulness\": evaluator_f,\n",
        "    \"relevancy\": evaluator_r,\n",
        "    \"semantic_similarity\": evaluator_s,\n",
        "}\n",
        "batch_runner = BatchEvalRunner(evaluator_dict, workers=2, show_progress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9abccb17-cf7d-4f59-b849-a4d5581df7a9",
      "metadata": {
        "id": "9abccb17-cf7d-4f59-b849-a4d5581df7a9"
      },
      "outputs": [],
      "source": [
        "eval_results = await batch_runner.aevaluate_responses(\n",
        "    eval_qs, responses=pred_responses, reference=ref_response_strs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c5df1ec-408a-4012-93c7-a0151fa92b9e",
      "metadata": {
        "id": "7c5df1ec-408a-4012-93c7-a0151fa92b9e"
      },
      "outputs": [],
      "source": [
        "base_eval_results = await batch_runner.aevaluate_responses(\n",
        "    eval_qs, responses=base_pred_responses, reference=ref_response_strs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea90f363-71d7-404c-9d4d-f6eea386e59f",
      "metadata": {
        "id": "ea90f363-71d7-404c-9d4d-f6eea386e59f",
        "outputId": "7d03198b-7681-47fc-fe75-c298f0ccbae7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>names</th>\n",
              "      <th>correctness</th>\n",
              "      <th>relevancy</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>semantic_similarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Auto Merging Retriever</td>\n",
              "      <td>4.266667</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.962196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Base Retriever</td>\n",
              "      <td>4.208333</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.960602</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    names  correctness  relevancy  faithfulness  \\\n",
              "0  Auto Merging Retriever     4.266667   0.916667          0.95   \n",
              "1          Base Retriever     4.208333   0.916667          0.95   \n",
              "\n",
              "   semantic_similarity  \n",
              "0             0.962196  \n",
              "1             0.960602  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "results_df = get_results_df(\n",
        "    [eval_results, base_eval_results],\n",
        "    [\"Auto Merging Retriever\", \"Base Retriever\"],\n",
        "    [\"correctness\", \"relevancy\", \"faithfulness\", \"semantic_similarity\"],\n",
        ")\n",
        "display(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be393093-9a6d-46d1-9d18-d17e92523200",
      "metadata": {
        "id": "be393093-9a6d-46d1-9d18-d17e92523200"
      },
      "source": [
        "**Analysis**: The results are roughly the same.\n",
        "\n",
        "Let's also try to see which answer GPT-4 prefers with our pairwise evals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91126247-b68d-43a9-b64f-f6764b0c2357",
      "metadata": {
        "id": "91126247-b68d-43a9-b64f-f6764b0c2357"
      },
      "outputs": [],
      "source": [
        "batch_runner = BatchEvalRunner(\n",
        "    {\"pairwise\": pairwise_evaluator}, workers=10, show_progress=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4426a26a-c26e-4560-b176-f2a7894adda7",
      "metadata": {
        "id": "4426a26a-c26e-4560-b176-f2a7894adda7"
      },
      "outputs": [],
      "source": [
        "pairwise_eval_results = await batch_runner.aevaluate_response_strs(\n",
        "    eval_qs,\n",
        "    response_strs=pred_response_strs,\n",
        "    reference=base_pred_response_strs,\n",
        ")\n",
        "pairwise_score = np.array(\n",
        "    [r.score for r in pairwise_eval_results[\"pairwise\"]]\n",
        ").mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0460557e-f80d-4dcb-a8ec-b08c61ed3129",
      "metadata": {
        "id": "0460557e-f80d-4dcb-a8ec-b08c61ed3129",
        "outputId": "4023a338-a01e-4d09-b52a-7426cc33e4cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.525"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pairwise_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ec2f94b-bbff-4fee-920a-bf4c872f23f2",
      "metadata": {
        "id": "9ec2f94b-bbff-4fee-920a-bf4c872f23f2"
      },
      "source": [
        "**Analysis**: The pairwise comparison score is a measure of the percentage of time the candidate answer (using auto-merging retriever) is preferred vs. the base answer (using the base retriever). Here we see that it's roughly even."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "a6bb584c",
        "outputId": "d9fb9951-ca3c-44ec-eb64-6aaba03640eb"
      },
      "source": [
        "from llama_index.core import Document\n",
        "\n",
        "doc_text = \"\\n\\n\".join([d.get_content() for d in docs0])\n",
        "docs = [Document(text=doc_text)]"
      ],
      "id": "a6bb584c",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'docs0' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3647442995.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdoc_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'docs0' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcff0f19"
      },
      "source": [
        "node_parser = HierarchicalNodeParser.from_defaults()\n",
        "nodes = node_parser.get_nodes_from_documents(docs)\n",
        "leaf_nodes = get_leaf_nodes(nodes)"
      ],
      "id": "dcff0f19",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8343f16"
      },
      "source": [
        "# define storage context\n",
        "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
        "from llama_index.core import StorageContext\n",
        "# from llama_index.llms.openai import OpenAI # Comment out OpenAI import\n",
        "from llama_index.llms.ollama import Ollama # Import Ollama for local LLM\n",
        "\n",
        "docstore = SimpleDocumentStore()\n",
        "\n",
        "# insert nodes into docstore\n",
        "docstore.add_documents(nodes)\n",
        "\n",
        "# define storage context (will include vector store by default too)\n",
        "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
        "\n",
        "# llm = OpenAI(model=\"gpt-3.5-turbo\") # Comment out OpenAI LLM\n",
        "llm = Ollama(model=\"llama2\") # Use Ollama with llama2 as an example local LLM"
      ],
      "id": "e8343f16",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "f051e836",
        "outputId": "9f2e6330-72cb-4fa5-e410-3ded92c9e836"
      },
      "source": [
        "## Load index into vector index\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.embeddings.ollama import OllamaEmbedding\n",
        "\n",
        "# Explicitly define a local embedding model\n",
        "embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
        "\n",
        "base_index = VectorStoreIndex(\n",
        "    leaf_nodes,\n",
        "    storage_context=storage_context,\n",
        "    embed_model=embed_model,  # Use the local embedding model\n",
        ")"
      ],
      "id": "f051e836",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'llama_index.embeddings.ollama'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-742065846.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Load index into vector index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorStoreIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mollama\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOllamaEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Explicitly define a local embedding model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index.embeddings.ollama'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}